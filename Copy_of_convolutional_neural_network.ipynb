{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgHkXY1qCNRc"
   },
   "source": [
    "no need for pandas and np as its an image dataset\n",
    "\n",
    "image sub module of preprocessing module import ImageDataGenerator\n",
    "\n",
    "ImageDataGenerator class is used for applying transformation on all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sCV30xyVhFbE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "FIleuCAjoFD8",
    "outputId": "9116597b-5d10-4518-b2de-75057a5a0256"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MSGtP6cC-6R"
   },
   "source": [
    "Apply transformations on all images of training set (wont apply on test set)\n",
    "\n",
    "This is to avoid overfitting.\n",
    "\n",
    "If we dont this there will be a huge difference in accuracy (98% in training set and much lower accuracy on test set)\n",
    "\n",
    "These transformations are shifting pixels, simple zooms and rotations (geometrical transformations)\n",
    "\n",
    "This is called IMAGE AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SFLZispPCujO"
   },
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nAHVpz2JzDu"
   },
   "source": [
    "zoom range zooms in or out of img\n",
    "\n",
    "horizontal flip flips the img\n",
    "\n",
    "shear_range is transvection(shifting pixels)\n",
    "\n",
    "rescale applies feature scaling to each and every one of our pixels by dividing the value by 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kravtf3Kh96"
   },
   "source": [
    "Now we need to connect the train_datagen object to our training set\n",
    "\n",
    "flow_from_directory imports our training set by accessing it from the directory while at the same time creates batches and resizes the images\n",
    "\n",
    "this is a method in ImageDataGenerator class\n",
    "\n",
    "class_mode='binary' or 'categorical', here cat or dog so binary\n",
    "\n",
    "first parameter is directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "U7dnUhpyMr0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set=train_datagen.flow_from_directory('dataset/training_set',target_size=(64, 64), batch_size=32, class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Va_1FiAGPHmR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255)\n",
    "test_set=test_datagen.flow_from_directory('dataset/test_set',target_size=(64, 64), batch_size=32, class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "78p97BayqPJ2"
   },
   "outputs": [],
   "source": [
    "cnn=tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmnw4Do3qhYy"
   },
   "source": [
    "convolutional layer is an object of Conv2D class\n",
    "\n",
    "filters is number of output filters in convolution\n",
    "\n",
    "classic number of filters is 32\n",
    "\n",
    "Kernel size is the number of rows (viz equal to num of columns) in feature detector (filter)\n",
    "\n",
    "eg: 3 --> 3x3\n",
    "\n",
    "activation = 'reLu'    rectifier in all steps except output\n",
    "\n",
    "color images input shape = (size,size,3)\n",
    "\n",
    "black and white images input shape = (size,size,1)\n",
    "\n",
    "where size is the size we put in target_size in flow_from_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UvS6eKDJqeed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhanj\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu',input_shape=(64,64,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Xa2Y94Pwqh3"
   },
   "source": [
    "pooling done using MaxPool2D class\n",
    "\n",
    "pool_size specifies the size of the frame that moves across the feature map\n",
    "\n",
    "padding - valid ignores 2 cells at the edge when frame goes out of bounds\n",
    "\n",
    "same create extra column that essentially hold the value 0\n",
    "\n",
    "recommended to keep default for padding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "KDSt4TFlvojR"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brYBIGNUx5Zl"
   },
   "source": [
    "input shape is specified only when first layer is being added\n",
    "\n",
    "No need to specify in extra convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "tXeNaIlYxwyu"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9to9MbqYyZUG"
   },
   "source": [
    "Flatten class doesnt take any parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "IHK8giM9yK5a"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7l5PLbpzz6R"
   },
   "source": [
    "as its more complex, larger number of neurons recommended\n",
    "\n",
    "generally recommended to use rectifier activation fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "jsOSS40syb5c"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWz4wAnH0HG7"
   },
   "source": [
    "binary classification hence 1 neuron\n",
    "\n",
    "sigmoid as its binary\n",
    "\n",
    "softmax for multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "cyKPPL4By-Qw"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "3WOmOrk80UcR"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYMr_h6C1pfW"
   },
   "source": [
    "training and evaluation done together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "72zmt7tD0szP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhanj\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 286ms/step - accuracy: 0.5453 - loss: 0.6873 - val_accuracy: 0.6775 - val_loss: 0.5974\n",
      "Epoch 2/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 189ms/step - accuracy: 0.6569 - loss: 0.6116 - val_accuracy: 0.5715 - val_loss: 0.6712\n",
      "Epoch 3/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 183ms/step - accuracy: 0.6950 - loss: 0.5719 - val_accuracy: 0.7280 - val_loss: 0.5441\n",
      "Epoch 4/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 184ms/step - accuracy: 0.7356 - loss: 0.5329 - val_accuracy: 0.7490 - val_loss: 0.5394\n",
      "Epoch 5/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 184ms/step - accuracy: 0.7543 - loss: 0.5107 - val_accuracy: 0.7630 - val_loss: 0.4925\n",
      "Epoch 6/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 188ms/step - accuracy: 0.7601 - loss: 0.4839 - val_accuracy: 0.7650 - val_loss: 0.5022\n",
      "Epoch 7/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 188ms/step - accuracy: 0.7847 - loss: 0.4503 - val_accuracy: 0.7495 - val_loss: 0.5352\n",
      "Epoch 8/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 184ms/step - accuracy: 0.7935 - loss: 0.4436 - val_accuracy: 0.7620 - val_loss: 0.5078\n",
      "Epoch 9/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 185ms/step - accuracy: 0.8083 - loss: 0.4219 - val_accuracy: 0.7900 - val_loss: 0.4614\n",
      "Epoch 10/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 183ms/step - accuracy: 0.8097 - loss: 0.4142 - val_accuracy: 0.7990 - val_loss: 0.4474\n",
      "Epoch 11/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 185ms/step - accuracy: 0.8112 - loss: 0.4044 - val_accuracy: 0.7735 - val_loss: 0.4834\n",
      "Epoch 12/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 171ms/step - accuracy: 0.8286 - loss: 0.3750 - val_accuracy: 0.7975 - val_loss: 0.4513\n",
      "Epoch 13/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - accuracy: 0.8431 - loss: 0.3622 - val_accuracy: 0.7835 - val_loss: 0.4921\n",
      "Epoch 14/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 192ms/step - accuracy: 0.8459 - loss: 0.3443 - val_accuracy: 0.7845 - val_loss: 0.5326\n",
      "Epoch 15/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 195ms/step - accuracy: 0.8315 - loss: 0.3637 - val_accuracy: 0.8015 - val_loss: 0.4670\n",
      "Epoch 16/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 194ms/step - accuracy: 0.8552 - loss: 0.3338 - val_accuracy: 0.7790 - val_loss: 0.5121\n",
      "Epoch 17/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 194ms/step - accuracy: 0.8660 - loss: 0.3096 - val_accuracy: 0.8115 - val_loss: 0.4584\n",
      "Epoch 18/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 195ms/step - accuracy: 0.8860 - loss: 0.2791 - val_accuracy: 0.7995 - val_loss: 0.4987\n",
      "Epoch 19/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 190ms/step - accuracy: 0.8861 - loss: 0.2785 - val_accuracy: 0.7775 - val_loss: 0.5850\n",
      "Epoch 20/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 160ms/step - accuracy: 0.8828 - loss: 0.2766 - val_accuracy: 0.7850 - val_loss: 0.5731\n",
      "Epoch 21/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 183ms/step - accuracy: 0.8920 - loss: 0.2617 - val_accuracy: 0.7875 - val_loss: 0.5541\n",
      "Epoch 22/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 159ms/step - accuracy: 0.9043 - loss: 0.2356 - val_accuracy: 0.8025 - val_loss: 0.5369\n",
      "Epoch 23/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 143ms/step - accuracy: 0.9042 - loss: 0.2305 - val_accuracy: 0.7960 - val_loss: 0.5456\n",
      "Epoch 24/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 185ms/step - accuracy: 0.9074 - loss: 0.2303 - val_accuracy: 0.7820 - val_loss: 0.6508\n",
      "Epoch 25/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 174ms/step - accuracy: 0.9131 - loss: 0.2107 - val_accuracy: 0.7840 - val_loss: 0.6409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e5e9136190>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set, validation_data=test_set, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6L2DX5z7Hs2"
   },
   "source": [
    "load_img convers to pil format\n",
    "\n",
    "this needs to be converted to a 2d array\n",
    "\n",
    "during training batches of 32 were used and not single images. This is an extra dimension\n",
    "\n",
    "CNN wasnt trained on single image but rather batches. Hence for predicting extra dimension needs to be added.\n",
    "\n",
    "we can minpulate this using np expand_dims()\n",
    "\n",
    "dimension of batch is first dimension, hence axis=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "9Xk5CDrS1lf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image=image.load_img('dataset/single_prediction/cat_or_dog_1.jpg',target_size=(64, 64))\n",
    "test_image=image.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "result=cnn.predict(test_image)\n",
    "if result[0][0]==1:                                       #first and only prediction in the batch\n",
    "  prediction='dog'\n",
    "else:\n",
    "  prediction='cat'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62QRAj97-E5o"
   },
   "source": [
    "to figure out what is 0 and 1, we call class_indices attribute from training set object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "yHpfsCJd-K0R"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
